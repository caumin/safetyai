{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cbc77b43",
      "metadata": {
        "id": "cbc77b43"
      },
      "source": [
        "## Dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1eedcc1",
      "metadata": {
        "id": "b1eedcc1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import math\n",
        "import random\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from skimage import io\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install addict\n",
        "import argparse, yaml\n",
        "from addict import Dict\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(\"Torchvision Version: \", torchvision.__version__)\n",
        "print(\"Pillow Version: \", PIL.__version__)\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb966666",
      "metadata": {
        "id": "cb966666"
      },
      "outputs": [],
      "source": [
        "#config 불러오기\n",
        "def get_cfg():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--config\", type=str, default=\"config.yaml\")\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    # args.config 에 지정된 YAML 파일 읽어서 Dict 형태로 반환\n",
        "    with open(args.config) as f:\n",
        "        cfg = Dict(yaml.safe_load(f))\n",
        "    return cfg\n",
        "\n",
        "cfg = get_cfg()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e0a2df",
      "metadata": {
        "id": "46e0a2df"
      },
      "outputs": [],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"alexattia/the-simpsons-characters-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b98216",
      "metadata": {
        "id": "37b98216"
      },
      "outputs": [],
      "source": [
        "train_dir = Path('/kaggle/input/the-simpsons-characters-dataset/simpsons_dataset')\n",
        "test_dir = Path('/kaggle/input/the-simpsons-characters-dataset/kaggle_simpson_testset/kaggle_simpson_testset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b611439",
      "metadata": {
        "id": "8b611439"
      },
      "outputs": [],
      "source": [
        "#데이터 전처리\n",
        "class SimpsonTrainValPath():\n",
        "\n",
        "  def __init__(self, train_dir, test_dir):\n",
        "\n",
        "    self.train_dir = train_dir\n",
        "    self.test_dir = test_dir\n",
        "    self.train_val_files_path = sorted(list(self.train_dir.rglob('*.jpg')))\n",
        "    self.test_path = sorted(list(self.test_dir.rglob('*.jpg')))\n",
        "    self.train_val_labels = [path.parent.name for path in self.train_val_files_path]\n",
        "\n",
        "  def get_path(self):\n",
        "\n",
        "    train_files_path, val_files_path = train_test_split(self.train_val_files_path, test_size = 0.3, random_state=cfg.data.random_state ,stratify=self.train_val_labels)\n",
        "\n",
        "    files_path = {'train': train_files_path, 'val': val_files_path}\n",
        "\n",
        "    return files_path, self.test_path\n",
        "\n",
        "  def get_n_classes(self):\n",
        "    return len(np.unique(self.train_val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4858a087",
      "metadata": {
        "id": "4858a087"
      },
      "outputs": [],
      "source": [
        "SimpsonTrainValPath = SimpsonTrainValPath(train_dir, test_dir)\n",
        "train_path, test_path = SimpsonTrainValPath.get_path()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5574991",
      "metadata": {
        "id": "d5574991"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, save_best_weights_path, save_last_weights_path, best_acc, num_epochs=cfg.training.epochs, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    val_loss_history = []\n",
        "    train_acc_history = []\n",
        "    train_loss_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # 모드 설정\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in tqdm_notebook(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # 훈련 모드에서 기록 저장\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # loss 계산\n",
        "                    # inception 모델인 경우\n",
        "                    if is_inception and phase == 'train':\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimizer train mode\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                #\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # loss\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # best 모델 저장\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            # acc 저장\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "                val_loss_history.append(epoch_loss)\n",
        "            else:\n",
        "                train_acc_history.append(epoch_acc)\n",
        "                train_loss_history.append(epoch_loss)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    history_val = {'loss': val_loss_history, 'acc': val_acc_history}\n",
        "    history_train = {'loss': train_loss_history, 'acc': train_acc_history}\n",
        "\n",
        "    return model, history_val, history_train, time_elapsed, best_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b5cf5f",
      "metadata": {
        "id": "f7b5cf5f"
      },
      "outputs": [],
      "source": [
        "# 저도 잘 모르겠어요\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "269ad56e",
      "metadata": {
        "id": "269ad56e"
      },
      "outputs": [],
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "\n",
        "    model_ft = None\n",
        "    input_size = cfg.data.input_size\n",
        "\n",
        "    if model_name == \"resnet152\":\n",
        "        \"\"\" Resnet152\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet152(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    elif model_name == \"resnet18\":\n",
        "        \"\"\" Resnet152\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abce1982",
      "metadata": {
        "id": "abce1982"
      },
      "outputs": [],
      "source": [
        "class SimpsonsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, files_path, data_transforms):\n",
        "      self.files_path = files_path\n",
        "      self.transform = data_transforms\n",
        "\n",
        "      if 'test' not in str(self.files_path[0]):\n",
        "        self.labels = [path.parent.name for path in self.files_path]\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.label_encoder.fit(self.labels)\n",
        "\n",
        "        with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
        "            pickle.dump(self.label_encoder, le_dump_file)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.files_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      img_path = str(self.files_path[idx])\n",
        "      image = Image.open(img_path)\n",
        "      image = self.transform(image)\n",
        "\n",
        "      if 'test' in str(self.files_path[0]):\n",
        "        return image\n",
        "      else:\n",
        "        label_str = str(self.files_path[idx].parent.name)\n",
        "        label = self.label_encoder.transform([label_str]).item()\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972a5c5a",
      "metadata": {
        "id": "972a5c5a"
      },
      "outputs": [],
      "source": [
        "fc_layer = 'all-st-SGD-m.9-nest-s-cycle-exp-.00001-.05-g.99994-m.8-.9'\n",
        "\n",
        "num_classes = SimpsonTrainValPath.get_n_classes()\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "# device 설정\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# feature_extract = False - обучаем всю модель\n",
        "# feature_extract = True - обучаем FC\n",
        "feature_extract = False\n",
        "\n",
        "# 가중치 저장\n",
        "save_last_weights_path = '/kaggle/working/' + cfg.model.name + '-' + fc_layer + '_last_weights.pth'\n",
        "save_best_weights_path = '/kaggle/working/' + cfg.model.name + '-' + fc_layer + '_best_weights.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35441278",
      "metadata": {
        "id": "35441278"
      },
      "outputs": [],
      "source": [
        "model_ft, input_size = initialize_model(cfg.model.name, num_classes, feature_extract=cfg.model.feature_extract, use_pretrained=cfg.model.pretrained)\n",
        "# GPU로 모델 올림림\n",
        "model_ft = model_ft.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c49a6f9",
      "metadata": {
        "id": "5c49a6f9"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.RandomChoice( [\n",
        "                                  transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                  transforms.ColorJitter(contrast=0.9),\n",
        "                                  transforms.ColorJitter(brightness=0.1),\n",
        "                                  transforms.RandomApply( [ transforms.RandomHorizontalFlip(p=1), transforms.ColorJitter(contrast=0.9) ], p=0.5),\n",
        "                                  transforms.RandomApply( [ transforms.RandomHorizontalFlip(p=1), transforms.ColorJitter(brightness=0.1) ], p=0.5),\n",
        "                                  ] ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4815bbdb",
      "metadata": {
        "id": "4815bbdb"
      },
      "outputs": [],
      "source": [
        "image_datasets = {mode: SimpsonsDataset(train_path[mode], data_transforms[mode]) for mode in ['train', 'val']}\n",
        "image_datasets_test = SimpsonsDataset(test_path, data_transforms['val'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d9578d3",
      "metadata": {
        "id": "3d9578d3"
      },
      "outputs": [],
      "source": [
        "cfg.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdffb0b7",
      "metadata": {
        "id": "fdffb0b7"
      },
      "outputs": [],
      "source": [
        "dataloaders_dict = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=cfg.data.batch_size, shuffle=True, num_workers=cfg.data.num_workers),\n",
        "                    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=cfg.data.batch_size, shuffle=True, num_workers=cfg.data.num_workers)}\n",
        "dataloader_test = torch.utils.data.DataLoader(image_datasets_test, batch_size=cfg.data.batch_size, shuffle=True, num_workers=cfg.data.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d127297",
      "metadata": {
        "id": "4d127297"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None, plt_ax=plt, default=False):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt_ax.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt_ax.set_title(title)\n",
        "    plt_ax.grid(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adf2dd33",
      "metadata": {
        "id": "adf2dd33"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(8, 8), \\\n",
        "                        sharey=True, sharex=True)\n",
        "for fig_x in ax.flatten():\n",
        "    random_characters = int(np.random.uniform(0, 4500))\n",
        "    im_val, label = image_datasets['train'][random_characters]\n",
        "    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
        "                image_datasets['val'].label_encoder.inverse_transform([label])[0].split('_')))\n",
        "    imshow(im_val.data.cpu(), \\\n",
        "          title=img_label,plt_ax=fig_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45eb5c90",
      "metadata": {
        "id": "45eb5c90"
      },
      "outputs": [],
      "source": [
        "def visualization(train, val, is_loss = True):\n",
        "\n",
        "  if is_loss:\n",
        "    plt.figure(figsize=(17,10))\n",
        "    plt.plot(train, label = 'Training loss')\n",
        "    plt.plot(val, label = 'Val loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  else:\n",
        "    plt.figure(figsize=(17,10))\n",
        "    plt.plot(train, label = 'Training acc')\n",
        "    plt.plot(val, label = 'Val acc')\n",
        "    plt.title('Training and validation acc')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Acc')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ce631a",
      "metadata": {
        "id": "f3ce631a"
      },
      "outputs": [],
      "source": [
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name, param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            pass\n",
        "            print(\"\\t\",name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3fcd843",
      "metadata": {
        "id": "e3fcd843"
      },
      "outputs": [],
      "source": [
        "model_ft, input_size = initialize_model(cfg.model.name, num_classes, feature_extract=cfg.model.feature_extract, use_pretrained=cfg.model.pretrained)\n",
        "model_ft = model_ft.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7771751",
      "metadata": {
        "id": "d7771751"
      },
      "outputs": [],
      "source": [
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name, param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            pass\n",
        "            print(\"\\t\",name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa3455a",
      "metadata": {
        "id": "5fa3455a"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=cfg.training.lr, momentum=cfg.training.momentum, nesterov = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e99fb14",
      "metadata": {
        "id": "2e99fb14"
      },
      "outputs": [],
      "source": [
        "val_loss = []\n",
        "val_acc = []\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "best_acc = .0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc013347",
      "metadata": {
        "id": "cc013347"
      },
      "outputs": [],
      "source": [
        "#모델 훈련\n",
        "\n",
        "total_epochs = cfg.training.epochs\n",
        "\n",
        "model_ft, hist_val, hist_train, time_elapsed, best_acc = train_model(\n",
        "    model_ft,\n",
        "    dataloaders_dict,\n",
        "    criterion,\n",
        "    optimizer_ft,\n",
        "    save_best_weights_path,\n",
        "    save_last_weights_path,\n",
        "    best_acc=0.0,\n",
        "    num_epochs=total_epochs,\n",
        "    is_inception=(cfg.model.name == \"inception\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45e811fb",
      "metadata": {
        "id": "45e811fb"
      },
      "outputs": [],
      "source": [
        "val_loss = hist_val['loss']\n",
        "val_acc = hist_val['acc']\n",
        "train_loss = hist_train['loss']\n",
        "train_acc = hist_train['acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d1a8acf",
      "metadata": {
        "id": "5d1a8acf"
      },
      "outputs": [],
      "source": [
        "train_acc_d = []\n",
        "for i in range(len(train_acc)):\n",
        "    train_acc_d.append(train_acc[i].cpu().data)\n",
        "\n",
        "val_acc_d = []\n",
        "for i in range(len(val_acc)):\n",
        "    val_acc_d.append(val_acc[i].cpu().data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92c643a",
      "metadata": {
        "id": "f92c643a"
      },
      "outputs": [],
      "source": [
        "visualization(train_acc_d, val_acc_d, is_loss = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "565ce821",
      "metadata": {
        "id": "565ce821"
      },
      "outputs": [],
      "source": [
        "visualization(train_loss, val_loss, is_loss = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5fa69e6",
      "metadata": {
        "id": "b5fa69e6"
      },
      "outputs": [],
      "source": [
        "def predict(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "\n",
        "        for inputs in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            model.eval()\n",
        "            outputs = model(inputs).cpu()\n",
        "            logits.append(outputs)\n",
        "\n",
        "    probs = nn.functional.softmax(torch.cat(logits), dim=1).numpy()\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d81d0b49",
      "metadata": {
        "id": "d81d0b49"
      },
      "outputs": [],
      "source": [
        "def predict_one_sample(model, img_tensor, device=device):\n",
        "    with torch.no_grad():\n",
        "        img_tensor = img_tensor.to(device)\n",
        "        model.eval()\n",
        "        y_hat = model(img_tensor).cpu()\n",
        "        y_pred = torch.nn.functional.softmax(y_hat, dim=1).numpy()\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c68193ca",
      "metadata": {
        "id": "c68193ca"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix():\n",
        "    # 실제 label 추출\n",
        "    actual = [image_datasets['val'][i][1] for i in range( len(image_datasets['val']) ) ]\n",
        "\n",
        "    # 이미지 추출\n",
        "    image = [image_datasets['val'][i][0] for i in range( len(image_datasets['val']) ) ]\n",
        "\n",
        "    img_conf_dataloader = torch.utils.data.DataLoader(image, batch_size=cfg.data.batch_size, shuffle=True, num_workers=cfg.data.num_workers)\n",
        "\n",
        "    probs = predict(model_ft, img_conf_dataloader)\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    # 테이블 생성\n",
        "    df = pd.DataFrame({'actual': actual, 'preds': preds})\n",
        "\n",
        "    confusion_matrix = pd.crosstab(df['actual'], df['preds'], rownames=['Actual'], colnames=['Predicted'], margins = False)\n",
        "\n",
        "    # 인코더 다운로드\n",
        "    label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n",
        "\n",
        "    # 클래스 목록 가져오기기\n",
        "    yticklabels = label_encoder.classes_\n",
        "\n",
        "    plt.subplots(figsize=(20,20))\n",
        "\n",
        "    sn.heatmap(confusion_matrix, annot=True, fmt=\"d\", linewidths=0.5, cmap=\"YlGnBu\", cbar=False, vmax = 30, yticklabels = yticklabels, xticklabels = yticklabels);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6158ed0",
      "metadata": {
        "id": "c6158ed0"
      },
      "outputs": [],
      "source": [
        "confusion_matrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f263b50",
      "metadata": {
        "id": "7f263b50"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as patches\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), \\\n",
        "                        sharey=True, sharex=True)\n",
        "\n",
        "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n",
        "\n",
        "for fig_x in ax.flatten():\n",
        "    random_characters = int(np.random.uniform(0, 1000))\n",
        "    im_val, label = image_datasets['val'][random_characters]\n",
        "    # inverse_transform은 LabelEncoder() 메서드로, 숫자에서 inverse_transform을 사용하여 클래스 이름을 반환\n",
        "    # 캐릭터 이름을 가져옴\n",
        "    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
        "                image_datasets['val'].label_encoder.inverse_transform([label])[0].split('_')))\n",
        "\n",
        "    imshow(im_val.data.cpu(), \\\n",
        "          title=img_label, plt_ax=fig_x)\n",
        "\n",
        "    actual_text = \"Actual : {}\".format(img_label)\n",
        "\n",
        "    # 확률 출력할 영역 추가\n",
        "    fig_x.add_patch(patches.Rectangle((0, 53), 86, 35, color='white'))\n",
        "    font0 = FontProperties()\n",
        "    font = font0.copy()\n",
        "    font.set_family(\"fantasy\")\n",
        "    prob_pred = predict_one_sample(model_ft, im_val.unsqueeze(0))\n",
        "    # 확률 가져오기\n",
        "    predicted_proba = np.max(prob_pred)*100\n",
        "    y_pred = np.argmax(prob_pred)\n",
        "\n",
        "    predicted_label = label_encoder.classes_[y_pred]\n",
        "    predicted_label = predicted_label[:len(predicted_label)//2] + '\\n' + predicted_label[len(predicted_label)//2:]\n",
        "    predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n",
        "\n",
        "    fig_x.text(1, 59, predicted_text , horizontalalignment='left', fontproperties=font,\n",
        "                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}