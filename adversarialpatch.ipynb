{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "import random\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from skimage import io\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse, yaml\n",
    "from addict import Dict\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"Torchvision Version: \", torchvision.__version__)\n",
    "print(\"Pillow Version: \", PIL.__version__)\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "path = \"checkpoint.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 불러오기\n",
    "model = models.resnet18(weights=None, num_classes=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18091f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized-Mean Pooling (GeM)\n",
    "# avgpool -> p-norm pooling\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3.0, eps=1e-6): \n",
    "        super().__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        return F.adaptive_avg_pool2d(x.clamp(min=self.eps).pow(self.p), (1,1)).pow(1./self.p)\n",
    "\n",
    "# 교체\n",
    "model.avgpool = GeM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\n",
    "    path,\n",
    "    map_location=torch.device('cpu'),\n",
    "    weights_only=True\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f65373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861284f",
   "metadata": {},
   "source": [
    "### Dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8011138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"alexattia/the-simpsons-characters-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = Path('/kaggle/input/the-simpsons-characters-dataset/simpsons_dataset/simpsons_dataset')\n",
    "test_dir = Path('/kaggle/input/the-simpsons-characters-dataset/kaggle_simpson_testset/kaggle_simpson_testset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValTestSplit():\n",
    "\n",
    "  def __init__(self, train_dir, test_dir):\n",
    "\n",
    "    self.train_dir = train_dir\n",
    "    self.test_dir = test_dir\n",
    "    # 하위 디렉토리를 순회하며 이미지의 경로를 리스트로 저장\n",
    "    self.train_val_files_path = sorted(list(self.train_dir.rglob('*.jpg')))\n",
    "    self.test_path = sorted(list(self.test_dir.rglob('*.jpg')))\n",
    "    self.train_val_labels = [path.parent.name for path in self.train_val_files_path]\n",
    "\n",
    "  def get_path(self):\n",
    "\n",
    "    train_files_path, val_files_path = train_test_split(self.train_val_files_path, test_size = 0.3, \\\n",
    "                                          stratify=self.train_val_labels, random_state = 42)\n",
    "\n",
    "    train_val_files_path = {'train': train_files_path, 'val': val_files_path}\n",
    "\n",
    "    return train_val_files_path, self.test_path\n",
    "\n",
    "  def get_n_classes(self):\n",
    "    return len(np.unique(self.train_val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainValTestPath = TrainValTestSplit(train_dir, test_dir)\n",
    "train_path, test_path = TrainValTestPath.get_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b26354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) -> imagenet 데이터셋의 통계 기반\n",
    "# TODO\n",
    "# simpson 데이터셋 전체의 std와 mean 계산 후 정규화에 이용\n",
    "\n",
    "input_size = 224\n",
    "\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)),\n",
    "        #transforms.CenterCrop(input_size),\n",
    "        transforms.RandomChoice( [\n",
    "                                  transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                  transforms.ColorJitter(contrast=0.9),\n",
    "                                  transforms.ColorJitter(brightness=0.1),\n",
    "                                  transforms.RandomApply( [ transforms.RandomHorizontalFlip(p=1), transforms.ColorJitter(contrast=0.9) ], p=0.5),\n",
    "                                  transforms.RandomApply( [ transforms.RandomHorizontalFlip(p=1), transforms.ColorJitter(brightness=0.1) ], p=0.5),\n",
    "                                  ] ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)),\n",
    "        #transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpsonsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, files_path, data_transforms):\n",
    "      self.files_path = files_path\n",
    "      self.transform = data_transforms\n",
    "\n",
    "      if 'test' not in str(self.files_path[0]):\n",
    "        self.labels = [path.parent.name for path in self.files_path]\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(self.labels)\n",
    "\n",
    "        with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
    "            pickle.dump(self.label_encoder, le_dump_file)\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.files_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "      img_path = str(self.files_path[idx])\n",
    "      image = Image.open(img_path)\n",
    "      image = self.transform(image)\n",
    "\n",
    "      if 'test' in str(self.files_path[0]):\n",
    "        return image\n",
    "      else:\n",
    "        label_str = str(self.files_path[idx].parent.name)\n",
    "        label = self.label_encoder.transform([label_str]).item()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbad2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {mode: SimpsonsDataset(train_path[mode], data_transforms[mode]) for mode in ['train', 'val']}\n",
    "image_datasets_test = SimpsonsDataset(test_path, data_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordker_id = 42\n",
    "num_workers = 0\n",
    "batch_size = 1\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True,\n",
    "                                                         num_workers=num_workers, worker_init_fn=seed_worker,generator=g),\n",
    "                    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True,\n",
    "                                                       num_workers=num_workers,worker_init_fn=seed_worker,generator=g)}\n",
    "dataloader_test = torch.utils.data.DataLoader(image_datasets_test, batch_size=batch_size, shuffle=False,\n",
    "                                              num_workers=num_workers, worker_init_fn=seed_worker,generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackbox_model(image_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    image_tensor: torch.Tensor, shape (1, 3, 224, 224), 이미 normalize된 상태\n",
    "    return: torch.Tensor, shape (num_classes,), softmax 확률\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits = model(image_tensor)\n",
    "        probs = F.softmax(logits, dim=1).squeeze(0)  # (num_classes,)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 50  # 50×50 픽셀 패치\n",
    "D = 3 * patch_size * patch_size  # 패치 파라미터 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_patch(theta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    flatten된 벡터 theta (길이 D)를 (3, patch_size, patch_size) 형태로 리쉐이프\n",
    "    \"\"\"\n",
    "    return theta.view(3, patch_size, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a21dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_patch_on_image(image: torch.Tensor,\n",
    "                           patch: torch.Tensor,\n",
    "                           top_left: tuple) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    image: (3, 224, 224), patch: (3, patch_size, patch_size)\n",
    "    top_left: (y, x) 좌표에 패치를 그대로 덮어씌움 (alpha blending 없음)\n",
    "    \"\"\"\n",
    "    patched = image.clone()\n",
    "    y0, x0 = top_left\n",
    "    h_p, w_p = patch_size, patch_size\n",
    "    patched[:, y0 : y0 + h_p, x0 : x0 + w_p] = patch\n",
    "    return patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_affine(patch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    patch: (3, patch_size, patch_size)\n",
    "    랜덤 회전, 랜덤 스케일, 랜덤 이동 없이 패치를 그대로 반환하거나,\n",
    "    필요하면 affine 변형을 적용해도 된다.\n",
    "    여기서는 변형 없이 리턴(가장 간단한 형태).\n",
    "    \"\"\"\n",
    "    return patch  # 변형이 필요하면 torchvision.transforms.functional.affine 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5               # 전체 에포크 수\n",
    "noise_samples = 20           # 매 iteration당 노이즈 샘플 개수 (n)\n",
    "sigma = 0.1                  # 노이즈 표준편차\n",
    "lr = 0.01                    # learning rate\n",
    "max_queries = 200000           # 최대 허용 쿼리 횟수\n",
    "query_count = 0              # 현재까지 사용된 쿼리 수 카운터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8150a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.rand(D, device=device)\n",
    "theta = theta.clamp(0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for img, true_label in tqdm_notebook(dataloaders_dict['train']):\n",
    "        if query_count >= max_queries:\n",
    "            break\n",
    "\n",
    "        img = img.to(device)               # (1, 3, 224, 224)\n",
    "        true_label = true_label.item()     # scalar\n",
    "\n",
    "        # 현재 패치 벡터 → 패치 텐서\n",
    "        base_patch = vector_to_patch(theta)  # (3, patch_size, patch_size)\n",
    "\n",
    "        # 배치 크기 1 처리이므로, 한 장의 이미지에만 패치 적용\n",
    "        # Affine 변형 없이 고정 위치에 붙여보겠다 (예: 좌상단 (0,0))\n",
    "        # 랜덤 위치에 붙이려면:\n",
    "        #   x0 = random.randint(0, 224 - patch_size)\n",
    "        #   y0 = random.randint(0, 224 - patch_size)\n",
    "        y0 = random.randint(0, 224 - patch_size)\n",
    "        x0 = random.randint(0, 224 - patch_size)\n",
    "        patched_img_base = overlay_patch_on_image(\n",
    "            image=img.squeeze(0),\n",
    "            patch=base_patch,\n",
    "            top_left=(y0, x0)\n",
    "        ).unsqueeze(0)  # (1,3,224,224)\n",
    "\n",
    "        # NES 노이즈 샘플링\n",
    "        noise = torch.randn((noise_samples, D), device=device) * sigma\n",
    "        grad_estimate = torch.zeros_like(theta)\n",
    "\n",
    "        for i in range(noise_samples):\n",
    "            if query_count >= max_queries:\n",
    "                break\n",
    "\n",
    "            eps = noise[i]                     # (D,)\n",
    "            theta_candidate = theta + eps\n",
    "            theta_candidate = theta_candidate.clamp(0.0, 1.0)\n",
    "\n",
    "            patch_candidate = vector_to_patch(theta_candidate)  # (3, patch_size, patch_size)\n",
    "\n",
    "            # 패치를 이미지에 적용 (랜덤 위치를 다시 샘플링해도 된다)\n",
    "            y1 = random.randint(0, 224 - patch_size)\n",
    "            x1 = random.randint(0, 224 - patch_size)\n",
    "            patched_img = overlay_patch_on_image(\n",
    "                image=img.squeeze(0),\n",
    "                patch=patch_candidate,\n",
    "                top_left=(y1, x1)\n",
    "            ).unsqueeze(0)  # (1,3,224,224)\n",
    "\n",
    "            # black-box 모델에 쿼리하여 확률 벡터 얻기\n",
    "            probs = blackbox_model(patched_img)  # (num_classes,)\n",
    "            query_count += 1\n",
    "\n",
    "            # Untargeted loss: true 클래스 확률을 최대한 낮추기\n",
    "            # Loss = + log(p_true)\n",
    "            loss_val = torch.log(probs[true_label] + 1e-10)\n",
    "\n",
    "            # gradient estimate 누적 (NES)\n",
    "            grad_estimate += loss_val * eps\n",
    "\n",
    "        # NES gradient normalize 및 업데이트 (부호는 +방향으로, true 확률을 줄이는 방향)\n",
    "        grad_estimate = grad_estimate / (noise_samples * sigma)\n",
    "        theta = theta + lr * grad_estimate    # gradient 상승 (loss를 키워야 p_true를 높이므로, \n",
    "                                               # untargeted일 때는 p_true를 높이는 방향이 패치가 망가지는 방향)\n",
    "        # 하지만 실제 untargeted 공격 목표는 p_true를 낮추는 것. \n",
    "        # NES 양식에서는 loss = log(p_true)이므로, loss를 크게 하면 p_true가 커짐.\n",
    "        # 따라서, 실제로 p_true를 낮추려면 loss = -log(p_true)으로 정의하고\n",
    "        #   theta = theta - lr * grad_estimate\n",
    "        # 처럼 부호 반대로 업데이트해야 한다. \n",
    "        # 하지만 여기서는 loss=log(p_true)이므로 grad_estimate 방향이 'p_true가 커지는 방향'임.\n",
    "        # 따라서 아래처럼 부호를 반전해야 함:\n",
    "        theta = theta.clamp(0.0, 1.0)\n",
    "        # (1) 패치 없이 원본 이미지의 true 확률\n",
    "        with torch.no_grad():\n",
    "            orig_prob = model(img.to(device))\n",
    "            orig_prob = F.softmax(orig_prob, dim=1)[0, true_label].item()\n",
    "\n",
    "        # (2) 패치 적용 후 true 확률\n",
    "        with torch.no_grad():\n",
    "            patched_prob = model(patched_img.to(device))\n",
    "            patched_prob = F.softmax(patched_prob, dim=1)[0, true_label].item()\n",
    "\n",
    "        # (3) 현재 iteration, loss, orig_prob, patched_prob 출력\n",
    "        print(f\"Iter {i:03d} | loss: {loss_val:.4f} | \"\n",
    "            f\"orig_true: {orig_prob:.4f} → patched_true: {patched_prob:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} 완료, 현재 쿼리 사용량: {query_count}/{max_queries}\")\n",
    "    if query_count >= max_queries:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbb1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_patch = vector_to_patch(theta).detach().cpu()  # (3, patch_size, patch_size)\n",
    "torch.save(final_patch, \"untargeted_adv_patch.pt\")\n",
    "print(\"최종 패치를 untargeted_adv_patch.pt로 저장했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dbe6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_tensor = torch.load(\"untargeted_adv_patch.pt\")  # shape: (3, patch_size, patch_size)\n",
    "print(patch_tensor.shape)  # 예: torch.Size([3, 50, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061dc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "# 2) 패치 텐서를 PIL 이미지로 바꿔서 시각화\n",
    "#    (patch_tensor이 [0,1] 범위여야 정상적으로 보입니다.)\n",
    "patch_pil = TF.to_pil_image(patch_tensor)  # (3, H_p, W_p) → PIL.Image\n",
    "patch_pil.show()  # 로컬 환경에서는 패치 이미지를 팝업 창으로 띄워줍니다.\n",
    "\n",
    "# 3) 원한다면 파일로 저장\n",
    "patch_pil.save(\"visualized_patch.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_sample(model, img_tensor, device=device):\n",
    "    with torch.no_grad():\n",
    "        img_tensor = img_tensor.to(device)\n",
    "        model.eval()\n",
    "        y_hat = model(img_tensor).cpu()\n",
    "        y_pred = torch.nn.functional.softmax(y_hat, dim=1).numpy()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceffdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None, plt_ax=plt, default=False):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt_ax.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt_ax.set_title(title)\n",
    "    plt_ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1331b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "\n",
    "# ── 2) 이미지 불러오기 및 전처리 ──\n",
    "#    (예시 경로: \"test_image.jpg\")\n",
    "img_pil = Image.open(\"C:/Users/alscj/.cache/kagglehub/datasets/alexattia/the-simpsons-characters-dataset/versions/4/kaggle_simpson_testset/kaggle_simpson_testset/charles_montgomery_burns_13.jpg\").convert(\"RGB\")\n",
    "preprocess = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "img_tensor = preprocess(img_pil).to(device)  # shape: (3, 224, 224)\n",
    "\n",
    "\n",
    "# ── 4) 패치 크기와 이미지 크기 정보 ──\n",
    "_, img_h, img_w = img_tensor.shape\n",
    "_, p_h, p_w = final_patch.shape\n",
    "\n",
    "# ── 5) 패치를 덮을 위치 결정 (랜덤 혹은 고정 위치) ──\n",
    "#    예시: 이미지 중앙에 붙이려면:\n",
    "x0 = (img_w - p_w) // 2\n",
    "y0 = (img_h - p_h) // 2\n",
    "\n",
    "#    또는 랜덤 위치 (0 ≤ x0 ≤ img_w - p_w, 0 ≤ y0 ≤ img_h - p_h)\n",
    "# x0 = random.randint(0, img_w - p_w)\n",
    "# y0 = random.randint(0, img_h - p_h)\n",
    "\n",
    "# ── 6) 패치 덮어씌우기 (덮어쓰기 방식) ──\n",
    "patched_img = img_tensor.clone()\n",
    "patched_img[:, y0 : y0 + p_h, x0 : x0 + p_w] = final_patch\n",
    "\n",
    "# ── 7) 배치 차원 추가 후 모델에 입력 → 확률 구하기 ──\n",
    "with torch.no_grad():\n",
    "    inp = patched_img.unsqueeze(0)  # (1, 3, 224, 224)\n",
    "    logits = model(inp)\n",
    "    probs = F.softmax(logits, dim=1).squeeze(0)  # (1000,)\n",
    "\n",
    "# ── 8) 원본 이미지와 패치 적용 이미지 비교 ──\n",
    "with torch.no_grad():\n",
    "    orig_logits = model(img_tensor.unsqueeze(0))\n",
    "    orig_probs = F.softmax(orig_logits, dim=1).squeeze(0)\n",
    "\n",
    "# ── 9) 결과 출력 ──\n",
    "top5_orig = torch.topk(orig_probs, 5)\n",
    "top5_patched = torch.topk(probs, 5)\n",
    "\n",
    "print(\"▶ 원본 이미지 Top-5 예측:\")\n",
    "for idx, prob in zip(top5_orig.indices, top5_orig.values):\n",
    "    print(f\"  클래스 {idx.item()}  확률 {prob.item():.4f}\")\n",
    "\n",
    "print(\"\\n▶ 패치 적용 후 이미지 Top-5 예측:\")\n",
    "for idx, prob in zip(top5_patched.indices, top5_patched.values):\n",
    "    print(f\"  클래스 {idx.item()}  확률 {prob.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer_name):\n",
    "        \"\"\"\n",
    "        model: PyTorch 분류 모델\n",
    "        target_layer_name: Grad-CAM을 구할 convolution 레이어 이름 (예: \"layer4\")\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.target_layer_name = target_layer_name\n",
    "\n",
    "        # forward hook, backward hook에서 기록할 placeholder\n",
    "        self.fmap = None\n",
    "        self.grad = None\n",
    "\n",
    "        # 지정된 레이어에 hook 등록\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name == self.target_layer_name:\n",
    "                module.register_forward_hook(self._forward_hook)\n",
    "                module.register_backward_hook(self._backward_hook)\n",
    "\n",
    "    def _forward_hook(self, module, input, output):\n",
    "        # forward 시, feature map(output)을 저장\n",
    "        self.fmap = output.detach()\n",
    "\n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        # backward 시, gradients(grad_output[0])를 저장\n",
    "        self.grad = grad_output[0].detach()\n",
    "\n",
    "    def generate(self, input_tensor, target_class=None):\n",
    "        \"\"\"\n",
    "        input_tensor: (1, 3, H, W) 형태, 이미 Normalize된 상태여야 함\n",
    "        target_class: None이면 모델이 예측한 top-1 클래스를 내부에서 사용\n",
    "        return:\n",
    "            heatmap_resized: (H, W) 형태의 numpy 배열(0~1 범위)\n",
    "            target_class: 실제로 사용된 class 인덱스\n",
    "        \"\"\"\n",
    "        # 1) forward → softmax 확률 구하기\n",
    "        logits = self.model(input_tensor)            # (1, num_classes)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        if target_class is None:\n",
    "            _, target_class = torch.max(probs, dim=1)\n",
    "            target_class = target_class.item()\n",
    "\n",
    "        # 2) 해당 클래스 스코어에 대해 backward\n",
    "        self.model.zero_grad()\n",
    "        score = logits[0, target_class]\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        # 3) hook으로 저장된 fmap, grad 가져오기\n",
    "        fmap = self.fmap[0]   # (C, h, w)\n",
    "        grad = self.grad[0]   # (C, h, w)\n",
    "\n",
    "        # 4) 채널별 평균 gradient → weight 계산\n",
    "        weights = torch.mean(grad.view(grad.shape[0], -1), dim=1)  # (C,)\n",
    "\n",
    "        # 5) fmap에 가중합\n",
    "        cam = torch.zeros(fmap.shape[1:], dtype=torch.float32, device=fmap.device)  # (h, w)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * fmap[i, :, :]\n",
    "\n",
    "        # 6) ReLU → normalize\n",
    "        cam = F.relu(cam)\n",
    "        cam -= cam.min()\n",
    "        if cam.max() > 0:\n",
    "            cam /= cam.max()\n",
    "        heatmap = cam.cpu().numpy()  # (h, w), 0~1\n",
    "\n",
    "        # 7) input_tensor의 원본 H, W 크기로 업샘플\n",
    "        _, _, H, W = input_tensor.shape\n",
    "        heatmap_resized = (\n",
    "            Image.fromarray(np.uint8(heatmap * 255))\n",
    "            .resize((W, H), resample=Image.BILINEAR)\n",
    "        )\n",
    "        heatmap_resized = np.array(heatmap_resized) / 255.0\n",
    "        return heatmap_resized, target_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# denormalize 함수 (imshow 시에 원본 색상을 복원하기 위해)\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std  = np.array([0.229, 0.224, 0.225])\n",
    "def denormalize_tensor(tensor):\n",
    "    \"\"\"\n",
    "    tensor: (3,H,W) 형태, 이미 Normalize된 텐서\n",
    "    return: (H, W, 3) numpy 배열(0~1 범위)\n",
    "    \"\"\"\n",
    "    img = tensor.cpu().permute(1, 2, 0).numpy()  # (H,W,3), normalized\n",
    "    img = (img * std) + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "# 3×3 subplot 그리드 준비\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=3, ncols=3,\n",
    "    figsize=(12, 12),\n",
    "    sharex=True, sharey=True\n",
    ")\n",
    "\n",
    "# 최종 생성된 패치 (정규화 기준이 동일해야 합니다)\n",
    "final_patch = torch.load(\"untargeted_adv_patch.pt\").to(device)  # (3, p_h, p_w)\n",
    "\n",
    "for subplot in ax.flatten():\n",
    "    # ── (1) 랜덤 샘플 고르기 ───────────────────────────\n",
    "    idx = int(np.random.uniform(0, len(image_datasets['val'])))\n",
    "    im_val, label = image_datasets['val'][idx]\n",
    "    # im_val: (3,224,224), 이미 Normalize된 텐서\n",
    "    # label: int\n",
    "\n",
    "    # ── (2) 패치 크기·위치 결정 ─────────────────────────\n",
    "    _, img_h, img_w = im_val.shape\n",
    "    _, p_h, p_w = final_patch.shape\n",
    "    x0 = random.randint(0, img_w - p_w)\n",
    "    y0 = random.randint(0, img_h - p_h)\n",
    "\n",
    "    # ── (3) 패치 덮기 ───────────────────────────────────\n",
    "    patched_img = im_val.clone()\n",
    "    patched_img[:, y0 : y0 + p_h, x0 : x0 + p_w] = final_patch\n",
    "\n",
    "    # ── (4) 바로 기존 predict_one_sample 호출 ─────────────────\n",
    "    # im_val.unsqueeze(0)가 (1,3,224,224) 형태, Normalize된 상태\n",
    "    prob_pred = predict_one_sample(model, im_val.unsqueeze(0))\n",
    "    # 기존 코드: 확률 가져오기\n",
    "    predicted_proba = np.max(prob_pred) * 100\n",
    "    y_pred = np.argmax(prob_pred)\n",
    "\n",
    "    predicted_label = image_datasets['val'].label_encoder.classes_[y_pred]\n",
    "    predicted_label = predicted_label[:len(predicted_label)//2] \\\n",
    "                      + predicted_label[len(predicted_label)//2:]\n",
    "    predicted_text = \"{} : {:.0f}%\".format(predicted_label, predicted_proba)\n",
    "\n",
    "    # ── (5) Grad-CAM heatmap 생성 ───────────────────────────\n",
    "    # patched_img.unsqueeze(0)는 (1,3,224,224), Normalize된 상태\n",
    "    \n",
    "    grad_cam = GradCAM(model, target_layer_name=\"layer4\")\n",
    "    \n",
    "    heatmap, target_cls = grad_cam.generate(\n",
    "        patched_img.unsqueeze(0).to(device),\n",
    "        target_class=None\n",
    "    )\n",
    "\n",
    "    # ── (6) 시각화 ─────────────────────────────────────────\n",
    "    # (A) 패치 적용된 이미지(denormalize) 띄우기\n",
    "    img_np = denormalize_tensor(patched_img)  # (224,224,3)\n",
    "    subplot.imshow(img_np)\n",
    "    subplot.axis(\"off\")\n",
    "\n",
    "    # (B) Grad-CAM heatmap을 jet 컬러맵으로 오버레이\n",
    "    subplot.imshow(heatmap, cmap=\"jet\", alpha=0.4)\n",
    "\n",
    "    # (C) 패치 위치를 흰색 테두리로 표시 (선택)\n",
    "    rect = patches.Rectangle(\n",
    "        (x0, y0), p_w, p_h,\n",
    "        linewidth=1, edgecolor=\"white\", facecolor=\"none\"\n",
    "    )\n",
    "    subplot.add_patch(rect)\n",
    "\n",
    "    # (D) 예측 확률 텍스트 및 실제 레이블 표시\n",
    "    subplot.add_patch(patches.Rectangle((0, 0), 140, 30, color=\"white\", alpha=0.7))\n",
    "    font0 = FontProperties()\n",
    "    font = font0.copy()\n",
    "    font.set_family(\"fantasy\")\n",
    "    subplot.text(\n",
    "        5, 18, predicted_text,\n",
    "        horizontalalignment=\"left\",\n",
    "        fontproperties=font,\n",
    "        verticalalignment=\"top\",\n",
    "        fontsize=9,\n",
    "        color=\"black\",\n",
    "        fontweight=\"bold\"\n",
    "    )\n",
    "    # 실제 레이블(Actual)도 필요하다면\n",
    "    actual_label_raw = image_datasets['val'].label_encoder.classes_[label]\n",
    "    actual_label = actual_label_raw[:len(actual_label_raw)//2] \\\n",
    "                   + actual_label_raw[len(actual_label_raw)//2:]\n",
    "    actual_text = f\"Actual: {actual_label}\"\n",
    "    subplot.text(\n",
    "        5, 36, actual_text,\n",
    "        horizontalalignment=\"left\",\n",
    "        fontproperties=font,\n",
    "        verticalalignment=\"top\",\n",
    "        fontsize=7,\n",
    "        color=\"gray\"\n",
    "    )\n",
    "\n",
    "# ── (7) 최종 레이아웃 정리 ───────────────────────────────────────\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ada86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), \\\n",
    "                        sharey=True, sharex=True)\n",
    "\n",
    "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n",
    "\n",
    "for fig_x in ax.flatten():\n",
    "      random_characters = int(np.random.uniform(0, 1000))\n",
    "      im_val, label = image_datasets['val'][random_characters]\n",
    "      # inverse_transform은 LabelEncoder() 메서드로, 숫자에서 inverse_transform을 사용하여 클래스 이름을 반환\n",
    "      # 캐릭터 이름을 가져옴\n",
    "      # ── 4) 패치 크기와 이미지 크기 정보 ──\n",
    "      _, img_h, img_w = im_val.shape\n",
    "      _, p_h, p_w = final_patch.shape\n",
    "\n",
    "      # ── 5) 패치를 덮을 위치 결정 (랜덤 혹은 고정 위치) ──\n",
    "      #    예시: 이미지 중앙에 붙이려면:\n",
    "      #x0 = (img_w - p_w) // 2\n",
    "      #y0 = (img_h - p_h) // 2\n",
    "\n",
    "      #    또는 랜덤 위치 (0 ≤ x0 ≤ img_w - p_w, 0 ≤ y0 ≤ img_h - p_h)\n",
    "      x0 = random.randint(0, img_w - p_w)\n",
    "      y0 = random.randint(0, img_h - p_h)\n",
    "\n",
    "      # ── 6) 패치 덮어씌우기 (덮어쓰기 방식) ──\n",
    "      patched_img = im_val.clone()\n",
    "      patched_img[:, y0 : y0 + p_h, x0 : x0 + p_w] = final_patch\n",
    "      img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
    "                  image_datasets['val'].label_encoder.inverse_transform([label])[0].split('_')))\n",
    "\n",
    "      imshow(patched_img.data.cpu(), \\\n",
    "            title=img_label, plt_ax=fig_x)\n",
    "\n",
    "      actual_text = \"Actual : {}\".format(img_label)\n",
    "\n",
    "      # 확률 출력할 영역 추가\n",
    "      fig_x.add_patch(patches.Rectangle((0, 53), 90, 35, color='white'))\n",
    "      font0 = FontProperties()\n",
    "      font = font0.copy()\n",
    "      font.set_family(\"fantasy\")\n",
    "      prob_pred = predict_one_sample(model, im_val.unsqueeze(0))\n",
    "      # 확률 가져오기\n",
    "      predicted_proba = np.max(prob_pred)*100\n",
    "      y_pred = np.argmax(prob_pred)\n",
    "\n",
    "      predicted_label = label_encoder.classes_[y_pred]\n",
    "      predicted_label = predicted_label[:len(predicted_label)//2] + predicted_label[len(predicted_label)//2:]\n",
    "      predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n",
    "\n",
    "      fig_x.text(1, 59, predicted_text , horizontalalignment='left', fontproperties=font,\n",
    "                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf526927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# ── (A) 레이블 인코더와 패치 로드 ─────────────────────────────────────────────────\n",
    "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))  # classes_ 길이 42\n",
    "final_patch = torch.load(\"untargeted_adv_patch.pt\").to(device)  # (3, p_h, p_w)\n",
    "\n",
    "# ── (B) 모델 불러오기 (앞서 A에서 설명) ──────────────────────────────────────────────\n",
    "# (model, device) 등이 이미 위에서 준비되어 있다고 가정\n",
    "\n",
    "# ── (C) Grad-CAM 클래스는 동일하게 사용 ────────────────────────────────────────────\n",
    "# (아래 GradCAM 코드는 앞선 예시와 동일하므로, 생략합니다. \n",
    "#  \"GradCAM\" 클래스를 그대로 복사해서 사용하시면 됩니다.)\n",
    "\n",
    "# (중략…) GradCAM 클래스 정의 …\n",
    "\n",
    "# ── (D) GradCAM 인스턴스 생성 ─────────────────────────────────────────────────────\n",
    "grad_cam = GradCAM(model, target_layer_name=\"layer4\")\n",
    "\n",
    "# ── (E) 3×3 Grid 시각화 루프 ────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=3, ncols=3,\n",
    "    figsize=(12, 12),\n",
    "    sharex=True, sharey=True\n",
    ")\n",
    "\n",
    "# Denormalize 함수도 앞선 예시 그대로 복사\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std  = np.array([0.229, 0.224, 0.225])\n",
    "def denormalize_tensor(tensor):\n",
    "    img = tensor.cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img * std) + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "for ax_i in ax.flatten():\n",
    "    # (1) 랜덤 샘플 추출\n",
    "    idx = int(np.random.uniform(0, len(image_datasets['val'])))\n",
    "    im_val, label = image_datasets['val'][idx]\n",
    "    # im_val: (3,224,224) 형태, 이미 Normalize 된 텐서\n",
    "    # label: 0~41 범위의 int\n",
    "    \n",
    "    # (2) 패치 적용 위치 결정\n",
    "    _, img_h, img_w = im_val.shape\n",
    "    _, p_h, p_w = final_patch.shape\n",
    "    x0 = random.randint(0, img_w - p_w)\n",
    "    y0 = random.randint(0, img_h - p_h)\n",
    "    \n",
    "    # (3) 패치 덮기\n",
    "    patched_img = im_val.clone()\n",
    "    patched_img[:, y0 : y0 + p_h, x0 : x0 + p_w] = final_patch\n",
    "    \n",
    "    # (4) 모델 예측 (패치 적용 이미지)\n",
    "    with torch.no_grad():\n",
    "        logits = model(patched_img.unsqueeze(0).to(device))  # (1,42)\n",
    "        probs = F.softmax(logits, dim=1).squeeze(0).cpu().numpy()  # (42,)\n",
    "    y_pred = np.argmax(probs)                 # 0~41 범위\n",
    "    predicted_proba = probs[y_pred] * 100      # % 단위\n",
    "    \n",
    "    # (5) 클래스 이름 복원\n",
    "    raw_pred_label = label_encoder.classes_[y_pred]    # ex. \"character_name\"\n",
    "    pred_label = \" \".join([w.capitalize() for w in raw_pred_label.split(\"_\")])\n",
    "    predicted_text = f\"{pred_label} : {predicted_proba:.0f}%\"\n",
    "    raw_true_label = label_encoder.classes_[label]\n",
    "    true_label_str = \" \".join([w.capitalize() for w in raw_true_label.split(\"_\")])\n",
    "    actual_text = f\"Actual: {true_label_str}\"\n",
    "    \n",
    "    # (6) Grad-CAM heatmap 계산\n",
    "    heatmap, target_cls = grad_cam.generate(\n",
    "        patched_img.unsqueeze(0).to(device),\n",
    "        target_class=None    # None이면 top-1 (y_pred) 사용\n",
    "    )\n",
    "    \n",
    "    # (7) Denormalize & 시각화\n",
    "    img_np = denormalize_tensor(patched_img)\n",
    "    ax_i.imshow(img_np)\n",
    "    ax_i.axis(\"off\")\n",
    "    \n",
    "    # (8) Grad-CAM heatmap 오버레이\n",
    "    ax_i.imshow(heatmap, cmap=\"jet\", alpha=0.4)\n",
    "    \n",
    "    # (9) 패치 위치 테두리 (선택)\n",
    "    rect = patches.Rectangle(\n",
    "        (x0, y0), p_w, p_h,\n",
    "        linewidth=1, edgecolor=\"white\", facecolor=\"none\"\n",
    "    )\n",
    "    ax_i.add_patch(rect)\n",
    "    \n",
    "    # (10) 예측/실제 라벨 텍스트 삽입\n",
    "    ax_i.add_patch(patches.Rectangle((0, 0), 140, 30, color=\"white\", alpha=0.7))\n",
    "    font0 = FontProperties()\n",
    "    font = font0.copy()\n",
    "    font.set_family(\"fantasy\")\n",
    "    ax_i.text(\n",
    "        5, 18, predicted_text,\n",
    "        horizontalalignment=\"left\",\n",
    "        fontproperties=font,\n",
    "        verticalalignment=\"top\",\n",
    "        fontsize=9,\n",
    "        color=\"black\",\n",
    "        fontweight=\"bold\"\n",
    "    )\n",
    "    ax_i.text(\n",
    "        5, 36, actual_text,\n",
    "        horizontalalignment=\"left\",\n",
    "        fontproperties=font,\n",
    "        verticalalignment=\"top\",\n",
    "        fontsize=7,\n",
    "        color=\"gray\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
